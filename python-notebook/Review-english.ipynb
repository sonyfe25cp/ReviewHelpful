{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datas.size:  100\n",
      "['A1004AX2J2HXGL', 'B00003CWPL', 'December 2, 2003', '111', '125', '5.0', '\"But what the hell do they know, I said?\"', ' If you\\'re reading this, then you\\'ve seen this movie or are at least curious what all the hype is about?    The late Stanley Kubrick, the only major filmmaker Lynch has cited as a direct cinematic influence, believed that ERASERHEAD was one of the most perfect \"cinematic experiences\" created to date.  This movie has enjoyed success on the midnight movie circuit for years, particularly in NYC where it ran almost every night for something like five years straight. I\\'ve seen it on big and little screens in three different states.  Insofar as interpretations are concerned, I\\'ve long since tossed all that out the window.  In terms of rational comprehension, ERASERHEAD is the fabled big fish that remains brilliantly elusive of any attempts to capture it.     This movie gets better, and more humorous, every time I watch it: in my opinion - ERASERHEAD is the cinematic experience that comes the closest to capturing \"dream logic\", next to the equally brilliant WAKING LIFE.  If you ever get the chance, watch ERASERHEAD in a movie theater with a great sound system - you will understand why Stanley Kubrick was moved enough to make his statement.  It\\'s like experiencing someone else\\'s dream - the ultimate act of voyeurism?  As if I was granted audience to a demonstration of delicate brain surgery, and catching glimpses of the patient\\'s face throughout the operation (particularly the opening scene).  It creates such a visceral landscape with its dark, peculiar selections of image and sound, that it seems to be constantly reminding you that the \"soul\" is helplessly sloshing around somewhere inside an organic bag of blood, bone, hair follicles, industrial shrapnel, dirt piles and antique radiators; a terrifying and beautiful delineation of a living creature suddenly made aware of its own being (birth imagery abounding).  It is a perfect symphony of sound and image, amazing work for a first time feature film director!  I\\'ve seen this movie placed in the HORROR section at local video stores; it\\'s better suited for the COMEDY section, I fear.  The movie was created on the AFI campus in California; production beginning his last year there, and continuing on for several more years in secret.  Not for everyone, but certainly worth a peek.    I own a copy of ERASERHEAD on DVD, finally.  It is available at David Lynch\\'s website for forty-five bucks plus shipping and handling.  Remastered sound and image, includes a few extras - the standout is a \"stylized\" interview with Lynch about the making of the film, the characters involved and anecdotes.    BTW - where is WILD AT HEART and LOST HIGHWAY for our DVD pleasure?     \\n', 0.89]\n"
     ]
    }
   ],
   "source": [
    "file_path = '/data/opinion_spam/real/reviewsNew.txt-utf8'\n",
    "datas = []\n",
    "MIN_FD = 100\n",
    "raw_file = open(file_path)\n",
    "debug = True\n",
    "debug_size = 100\n",
    "_debug_count = 0\n",
    "for line in raw_file:    \n",
    "    try:\n",
    "        line = line.decode('gbk')\n",
    "    except:\n",
    "        continue\n",
    "    #print 'gbk', line\n",
    "    line = line.encode('utf-8')\n",
    "    #print 'utf8', line\n",
    "    item = line.split('\\t')\n",
    "    if len(item) == 8:\n",
    "        hfd = float(item[3])\n",
    "        fd = float(item[4])\n",
    "        ratings = float(item[5])\n",
    "        if fd > MIN_FD:\n",
    "            hfdfd = round(hfd/fd, 2)\n",
    "            #whole_filted_body += item[7]\n",
    "            item.append(hfdfd)\n",
    "            datas.append(item)\n",
    "            if debug == True:\n",
    "                _debug_count += 1\n",
    "                if _debug_count >= debug_size:\n",
    "                    break\n",
    "raw_file.close()\n",
    "print 'datas.size: ', len(datas)\n",
    "print datas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rid</th>\n",
       "      <th>pid</th>\n",
       "      <th>date</th>\n",
       "      <th>hfd</th>\n",
       "      <th>fd</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>hfdfd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1004AX2J2HXGL</td>\n",
       "      <td>B00003CWPL</td>\n",
       "      <td>December 2, 2003</td>\n",
       "      <td>111</td>\n",
       "      <td>125</td>\n",
       "      <td>5.0</td>\n",
       "      <td>\"But what the hell do they know, I said?\"</td>\n",
       "      <td>If you're reading this, then you've seen this...</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A103PHKSEPT10R</td>\n",
       "      <td>0939484463</td>\n",
       "      <td>July 5, 2000</td>\n",
       "      <td>69</td>\n",
       "      <td>116</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Credible, Interesting, &amp; Devasting</td>\n",
       "      <td>I've read Prof Butz's book twice and an still...</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A103U0Q3IKSXHE</td>\n",
       "      <td>0875845851</td>\n",
       "      <td>January 29, 2000</td>\n",
       "      <td>111</td>\n",
       "      <td>115</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Identifying the horns of the dilemma.</td>\n",
       "      <td>Prior to reading this book, I chalked up the ...</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A105YVLAZNYQUU</td>\n",
       "      <td>B000634DCW</td>\n",
       "      <td>June 21, 2005</td>\n",
       "      <td>6</td>\n",
       "      <td>112</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UGGHHH</td>\n",
       "      <td>Craptacular and boring. The cinematography an...</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1075MZNVRMSEO</td>\n",
       "      <td>1563249367</td>\n",
       "      <td>April 2, 2002</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A historical portrait &amp; revelation- un chin de...</td>\n",
       "      <td>This book is a fascinating journey through th...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rid         pid              date  hfd   fd rating  \\\n",
       "0  A1004AX2J2HXGL  B00003CWPL  December 2, 2003  111  125    5.0   \n",
       "1  A103PHKSEPT10R  0939484463      July 5, 2000   69  116    5.0   \n",
       "2  A103U0Q3IKSXHE  0875845851  January 29, 2000  111  115    4.0   \n",
       "3  A105YVLAZNYQUU  B000634DCW     June 21, 2005    6  112    1.0   \n",
       "4  A1075MZNVRMSEO  1563249367     April 2, 2002  190  190    5.0   \n",
       "\n",
       "                                               title  \\\n",
       "0          \"But what the hell do they know, I said?\"   \n",
       "1                 Credible, Interesting, & Devasting   \n",
       "2              Identifying the horns of the dilemma.   \n",
       "3                                             UGGHHH   \n",
       "4  A historical portrait & revelation- un chin de...   \n",
       "\n",
       "                                                body  hfdfd  \n",
       "0   If you're reading this, then you've seen this...   0.89  \n",
       "1   I've read Prof Butz's book twice and an still...   0.59  \n",
       "2   Prior to reading this book, I chalked up the ...   0.97  \n",
       "3   Craptacular and boring. The cinematography an...   0.05  \n",
       "4   This book is a fascinating journey through th...   1.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set column names\n",
    "column_names = ['rid', 'pid', 'date', 'hfd', 'fd', 'rating', 'title', 'body', 'hfdfd']\n",
    "df = pd.DataFrame(datas)\n",
    "df.columns = column_names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transfer the type of column\n",
    "df[['hfd', 'fd','rating']] = df[['hfd', 'fd', 'rating']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hfd</th>\n",
       "      <th>fd</th>\n",
       "      <th>rating</th>\n",
       "      <th>hfdfd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>129.690000</td>\n",
       "      <td>159.060000</td>\n",
       "      <td>4.14000</td>\n",
       "      <td>0.801700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>115.687291</td>\n",
       "      <td>118.980163</td>\n",
       "      <td>1.42148</td>\n",
       "      <td>0.261565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>92.750000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>0.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>106.500000</td>\n",
       "      <td>124.500000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>137.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>966.000000</td>\n",
       "      <td>1065.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              hfd           fd     rating       hfdfd\n",
       "count  100.000000   100.000000  100.00000  100.000000\n",
       "mean   129.690000   159.060000    4.14000    0.801700\n",
       "std    115.687291   118.980163    1.42148    0.261565\n",
       "min      6.000000   101.000000    1.00000    0.050000\n",
       "25%     92.750000   109.000000    4.00000    0.727500\n",
       "50%    106.500000   124.500000    5.00000    0.930000\n",
       "75%    137.000000   158.000000    5.00000    0.980000\n",
       "max    966.000000  1065.000000    5.00000    1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fe41afc84d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEPCAYAAABP1MOPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe41b472e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hfdfd distribution\n",
    "df.hfdfd.hist()\n",
    "plt.title='The Distribute of hfd/fd'\n",
    "plt.xlabel('Score of hfd/fd')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fe41ae25890>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEPCAYAAABP1MOPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe41b481510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#rating distribution\n",
    "df.rating.hist()\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "different product size: 100\n"
     ]
    }
   ],
   "source": [
    "#count how many products in the dataset  ==ã€‹9809 products\n",
    "df.groupby('pid').count().shape\n",
    "print 'different product size:', df.groupby('pid').count().shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['title_len'] = df['title'].apply(len)\n",
    "df['body_len'] = df['body'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rid</th>\n",
       "      <th>pid</th>\n",
       "      <th>date</th>\n",
       "      <th>hfd</th>\n",
       "      <th>fd</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>hfdfd</th>\n",
       "      <th>title_len</th>\n",
       "      <th>body_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1004AX2J2HXGL</td>\n",
       "      <td>B00003CWPL</td>\n",
       "      <td>December 2, 2003</td>\n",
       "      <td>111</td>\n",
       "      <td>125</td>\n",
       "      <td>5</td>\n",
       "      <td>\"But what the hell do they know, I said?\"</td>\n",
       "      <td>If you're reading this, then you've seen this...</td>\n",
       "      <td>0.89</td>\n",
       "      <td>41</td>\n",
       "      <td>2642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A103PHKSEPT10R</td>\n",
       "      <td>0939484463</td>\n",
       "      <td>July 5, 2000</td>\n",
       "      <td>69</td>\n",
       "      <td>116</td>\n",
       "      <td>5</td>\n",
       "      <td>Credible, Interesting, &amp; Devasting</td>\n",
       "      <td>I've read Prof Butz's book twice and an still...</td>\n",
       "      <td>0.59</td>\n",
       "      <td>34</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A103U0Q3IKSXHE</td>\n",
       "      <td>0875845851</td>\n",
       "      <td>January 29, 2000</td>\n",
       "      <td>111</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>Identifying the horns of the dilemma.</td>\n",
       "      <td>Prior to reading this book, I chalked up the ...</td>\n",
       "      <td>0.97</td>\n",
       "      <td>37</td>\n",
       "      <td>1495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A105YVLAZNYQUU</td>\n",
       "      <td>B000634DCW</td>\n",
       "      <td>June 21, 2005</td>\n",
       "      <td>6</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>UGGHHH</td>\n",
       "      <td>Craptacular and boring. The cinematography an...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1075MZNVRMSEO</td>\n",
       "      <td>1563249367</td>\n",
       "      <td>April 2, 2002</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>5</td>\n",
       "      <td>A historical portrait &amp; revelation- un chin de...</td>\n",
       "      <td>This book is a fascinating journey through th...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>59</td>\n",
       "      <td>2180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rid         pid              date  hfd   fd  rating  \\\n",
       "0  A1004AX2J2HXGL  B00003CWPL  December 2, 2003  111  125       5   \n",
       "1  A103PHKSEPT10R  0939484463      July 5, 2000   69  116       5   \n",
       "2  A103U0Q3IKSXHE  0875845851  January 29, 2000  111  115       4   \n",
       "3  A105YVLAZNYQUU  B000634DCW     June 21, 2005    6  112       1   \n",
       "4  A1075MZNVRMSEO  1563249367     April 2, 2002  190  190       5   \n",
       "\n",
       "                                               title  \\\n",
       "0          \"But what the hell do they know, I said?\"   \n",
       "1                 Credible, Interesting, & Devasting   \n",
       "2              Identifying the horns of the dilemma.   \n",
       "3                                             UGGHHH   \n",
       "4  A historical portrait & revelation- un chin de...   \n",
       "\n",
       "                                                body  hfdfd  title_len  \\\n",
       "0   If you're reading this, then you've seen this...   0.89         41   \n",
       "1   I've read Prof Butz's book twice and an still...   0.59         34   \n",
       "2   Prior to reading this book, I chalked up the ...   0.97         37   \n",
       "3   Craptacular and boring. The cinematography an...   0.05          6   \n",
       "4   This book is a fascinating journey through th...   1.00         59   \n",
       "\n",
       "   body_len  \n",
       "0      2642  \n",
       "1       313  \n",
       "2      1495  \n",
       "3      1045  \n",
       "4      2180  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fe41ad9a9d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEPCAYAAACk43iMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe41adc6850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['title_len'].hist()\n",
    "plt.xlabel('length of review title')\n",
    "plt.ylabel('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fe41ad348d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEPCAYAAACp/QjLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe41ac15c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[df['body_len'] > 10][df['body_len'] < 5000]['body_len'].hist()\n",
    "plt.xlim(0, 5000)\n",
    "plt.xlabel(\"length of review body\")\n",
    "plt.ylabel(\"counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_len</th>\n",
       "      <th>body_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.810000</td>\n",
       "      <td>2190.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.042495</td>\n",
       "      <td>1157.861036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>313.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>1342.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>2024.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2882.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>5397.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title_len     body_len\n",
       "count  100.000000   100.000000\n",
       "mean    35.810000  2190.390000\n",
       "std     15.042495  1157.861036\n",
       "min      3.000000   313.000000\n",
       "25%     24.000000  1342.500000\n",
       "50%     35.000000  2024.000000\n",
       "75%     48.000000  2882.000000\n",
       "max     80.000000  5397.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['title_len', 'body_len']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tfidf matrix size :  (100, 1856)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "#TFIDF feature without lsa\n",
    "\n",
    "import evaluation_self as es\n",
    "max_features_count = 10000\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=max_features_count,\n",
    "                                 min_df=2, stop_words='english', tokenizer = es.tokenize_and_stem)\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(df.body)\n",
    "print \"the tfidf matrix size : \",tfidf_matrix.shape\n",
    "print type(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF-cv10-LSVR, mse: [ 0.08141925  0.04912799  0.10494847  0.04176103  0.06310264  0.04258743\n",
      "  0.0500274   0.03956934  0.01890513  0.07190517]\n",
      "TFIDF-cv10-LSVR, mse mean: 0.0563353850276\n",
      "TFIDF-cv10-LSVR, rmse: [ 0.28534059  0.22164834  0.32395751  0.20435515  0.25120238  0.20636721\n",
      "  0.22366807  0.19892043  0.13749594  0.2681514 ]\n",
      "TFIDF-cv10-LSVR, rmse mean: 0.232110703073\n"
     ]
    }
   ],
   "source": [
    "#TFIDF-CV-10-lsvr\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "lsvr = LinearSVR()\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores2 = cross_val_score(lsvr, tfidf_matrix, df.hfdfd, cv=10, scoring=\"mean_squared_error\")\n",
    "\n",
    "mse2 = abs(scores2)\n",
    "print 'TFIDF-cv10-LSVR, mse:', mse2\n",
    "print 'TFIDF-cv10-LSVR, mse mean:', np.mean(mse2)\n",
    "print 'TFIDF-cv10-LSVR, rmse:', np.sqrt(mse2)\n",
    "print 'TFIDF-cv10-LSVR, rmse mean:', np.mean(np.sqrt(mse2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF-cv10-SVR, mse: [ 0.10782484  0.03885816  0.15268398  0.01068192  0.0971278   0.07590734\n",
      "  0.07890349  0.05963978  0.00829349  0.10984222]\n",
      "TFIDF-cv10-SVR, mse mean: 0.0739763006971\n",
      "TFIDF-cv10-SVR, rmse: [ 0.32836693  0.19712472  0.39074797  0.10335338  0.31165334  0.27551286\n",
      "  0.28089765  0.24421256  0.09106859  0.33142453]\n",
      "TFIDF-cv10-SVR, rmse mean: 0.255436253884\n"
     ]
    }
   ],
   "source": [
    "#TFIDF-CV-10-svr\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores2 = cross_val_score(svr, tfidf_matrix, df.hfdfd, cv=10, scoring=\"mean_squared_error\")\n",
    "\n",
    "mse2 = abs(scores2)\n",
    "print 'TFIDF-cv10-SVR, mse:', mse2\n",
    "print 'TFIDF-cv10-SVR, mse mean:', np.mean(mse2)\n",
    "print 'TFIDF-cv10-SVR, rmse:', np.sqrt(mse2)\n",
    "print 'TFIDF-cv10-SVR, rmse mean:', np.mean(np.sqrt(mse2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF-cv10-LR, mse: [ 0.08428988  0.04914983  0.10853811  0.03688545  0.06363214  0.04363286\n",
      "  0.05190089  0.0389887   0.01494012  0.07207759]\n",
      "TFIDF-cv10-LR, mse mean: 0.0564035564685\n",
      "TFIDF-cv10-LR, rmse: [ 0.29032719  0.22169761  0.32945123  0.19205586  0.25225411  0.2088848\n",
      "  0.22781766  0.19745556  0.12222977  0.2684727 ]\n",
      "TFIDF-cv10-LR, rmse mean: 0.231064649804\n"
     ]
    }
   ],
   "source": [
    "#TFIDF-CV-10-lr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores2 = cross_val_score(lr, tfidf_matrix, df.hfdfd, cv=10, scoring=\"mean_squared_error\")\n",
    "\n",
    "mse2 = abs(scores2)\n",
    "print 'TFIDF-cv10-LR, mse:', mse2\n",
    "print 'TFIDF-cv10-LR, mse mean:', np.mean(mse2)\n",
    "print 'TFIDF-cv10-LR, rmse:', np.sqrt(mse2)\n",
    "print 'TFIDF-cv10-LR, rmse mean:', np.mean(np.sqrt(mse2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF - LSA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tfidf matrix size :  (100, 1856)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "Performing dimensionality reduction using LSA\n",
      "done in  4.06745004654\n",
      "Explained variance of the SVD step: 100\n"
     ]
    }
   ],
   "source": [
    "#TFIDF feature without lsa\n",
    "from time import time\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import evaluation_self as es\n",
    "max_features_count = 10000\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=max_features_count,\n",
    "                                 min_df=2, stop_words='english', tokenizer = es.tokenize_and_stem)\n",
    "lsa_tfidf_matrix = vectorizer.fit_transform(df.body)\n",
    "print \"the tfidf matrix size : \",tfidf_matrix.shape\n",
    "print type(tfidf_matrix)\n",
    "\n",
    "lsaornot = True\n",
    "lsanum = 1000\n",
    "\n",
    "if lsaornot:\n",
    "    print \"Performing dimensionality reduction using LSA\"\n",
    "    t0 = time()\n",
    "    # Vectorizer results are normalized, which makes KMeans behave as\n",
    "    # spherical k-means for better results. Since LSA/SVD results are\n",
    "    # not normalized, we have to redo the normalization.\n",
    "    svd = TruncatedSVD(lsanum)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    lsa_tfidf_matrix = lsa.fit_transform(tfidf_matrix)\n",
    "\n",
    "    print \"done in \", (time() - t0)\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print \"Explained variance of the SVD step:\", int(explained_variance * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF-LSA-cv10-LR, mse: [ 0.08006862  0.04964595  0.10925664  0.03678711  0.10221044  0.03274556\n",
      "  0.05157836  0.07870043  0.01658272  0.10811737]\n",
      "TFIDF-LSA-cv10-LR, mse mean: 0.0665693193636\n",
      "TFIDF-LSA-cv10-LR, rmse2: [ 0.282964    0.2228137   0.33053992  0.19179967  0.31970367  0.18095735\n",
      "  0.2271087   0.28053597  0.12877389  0.32881206]\n",
      "TFIDF-LSA-cv10-LR, rmse2 mean: 0.249400892182\n"
     ]
    }
   ],
   "source": [
    "#TFIDF-LSA-cv10-LR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores2 = cross_val_score(lr, lsa_tfidf_matrix, df.hfdfd, cv=10, scoring=\"mean_squared_error\")\n",
    "\n",
    "mse2 = abs(scores2)\n",
    "print 'TFIDF-LSA-cv10-LR, mse:', mse2\n",
    "print 'TFIDF-LSA-cv10-LR, mse mean:', np.mean(mse2)\n",
    "print 'TFIDF-LSA-cv10-LR, rmse2:', np.sqrt(mse2)\n",
    "print 'TFIDF-LSA-cv10-LR, rmse2 mean:', np.mean(np.sqrt(mse2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF-cv10-LSVR, mse: [ 0.08141942  0.0491236   0.10494618  0.04175843  0.0631031   0.04258581\n",
      "  0.05002453  0.03957113  0.01890863  0.07190426]\n",
      "TFIDF-cv10-LSVR, mse mean: 0.0563345091237\n",
      "TFIDF-cv10-LSVR, rmse: [ 0.28534089  0.22163843  0.32395398  0.20434881  0.2512033   0.20636329\n",
      "  0.22366164  0.19892494  0.13750865  0.2681497 ]\n",
      "TFIDF-cv10-LSVR, rmse mean: 0.232109363337\n"
     ]
    }
   ],
   "source": [
    "#TFIDF-LSA-CV-10-lsvr\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "lsvr = LinearSVR()\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores2 = cross_val_score(lsvr, lsa_tfidf_matrix, df.hfdfd, cv=10, scoring=\"mean_squared_error\")\n",
    "\n",
    "mse2 = abs(scores2)\n",
    "print 'TFIDF-cv10-LSVR, mse:', mse2\n",
    "print 'TFIDF-cv10-LSVR, mse mean:', np.mean(mse2)\n",
    "print 'TFIDF-cv10-LSVR, rmse:', np.sqrt(mse2)\n",
    "print 'TFIDF-cv10-LSVR, rmse mean:', np.mean(np.sqrt(mse2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF-cv10-SVR, mse: [ 0.10253726  0.03873028  0.14433088  0.01243679  0.09113803  0.07184335\n",
      "  0.07314219  0.0558068   0.00898287  0.1030408 ]\n",
      "TFIDF-cv10-SVR, mse mean: 0.0701989240622\n",
      "TFIDF-cv10-SVR, rmse: [ 0.3202144   0.1968001   0.37990904  0.11152036  0.30189076  0.26803609\n",
      "  0.27044813  0.23623462  0.094778    0.32099969]\n",
      "TFIDF-cv10-SVR, rmse mean: 0.250083118278\n",
      "tfidf-svr cost: 0.0429480075836\n"
     ]
    }
   ],
   "source": [
    "#TFIDF-LSA-CV-10-svr\n",
    "t0 = time()\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores2 = cross_val_score(svr, lsa_tfidf_matrix, df.hfdfd, cv=10, scoring=\"mean_squared_error\")\n",
    "\n",
    "mse2 = abs(scores2)\n",
    "print 'TFIDF-cv10-SVR, mse:', mse2\n",
    "print 'TFIDF-cv10-SVR, mse mean:', np.mean(mse2)\n",
    "print 'TFIDF-cv10-SVR, rmse:', np.sqrt(mse2)\n",
    "print 'TFIDF-cv10-SVR, rmse mean:', np.mean(np.sqrt(mse2))\n",
    "print \"tfidf-svr cost:\", (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STD features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 3, 100.33333333333333, 0, 0]\n",
      "[152, 12, 122.08333333333333, 0, 0]\n",
      "[118, 12, 85.333333333333329, 0, 3]\n",
      "[173, 11, 196.36363636363637, 1, 0]\n",
      "[122, 15, 77.933333333333337, 1, 1]\n",
      "[255, 21, 134.38095238095238, 0, 1]\n",
      "[47, 3, 101.0, 1, 0]\n",
      "[110, 6, 155.16666666666666, 0, 0]\n",
      "[151, 13, 117.84615384615384, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "#STD features\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "def seg_sentence(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return [s.strip() for s in sentences if(len(s)) > 0]\n",
    "\n",
    "def marks_num(text, pattern):\n",
    "    return len(re.findall(pattern, text))\n",
    "\n",
    "std_exc = 0\n",
    "def std(text): # std in Yang@acl-2015\n",
    "#    print '==============================='\n",
    "    ary = [0, 0, 0, 0, 0]\n",
    "    try:\n",
    "        tokens = es.tokenize_and_stem(text)\n",
    "        token_num = len(tokens)\n",
    "    #    print 'token size:', token_num\n",
    "        sentences = seg_sentence(text)\n",
    "        sen_num = len(sentences)\n",
    "    #    print 'sentence size:', sen_num\n",
    "        avg_sentence_len = np.mean([len(x) for x in sentences])\n",
    "    #    print 'avg sentence length:', avg_sentence_len\n",
    "        exc_num = marks_num(text, r'[!ï¼]')\n",
    "    #    print 'exclamation marks count:', exc_num\n",
    "        que_num = marks_num(text, r'[ï¼Ÿ?]')\n",
    "    #    print 'question percentage:', float(que_num) / sen_num\n",
    "        ary = [token_num, sen_num, avg_sentence_len, exc_num, que_num]\n",
    "#         print ary\n",
    "    except:\n",
    "#         print text\n",
    "        std_exc += 1\n",
    "    finally:\n",
    "        return ary\n",
    "    \n",
    "for loc in range(1, 10):\n",
    "    print std(df.body.head(10)[loc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "std exception:  0\n",
      "<type 'numpy.ndarray'>\n",
      "(100, 5)\n"
     ]
    }
   ],
   "source": [
    "#generate std features\n",
    "\n",
    "\n",
    "std_x = np.array(df.body)\n",
    "print type(std_x)\n",
    "std_x = np.array(map(std, std_x))\n",
    "print \"std exception: \", std_exc\n",
    "\n",
    "print type(std_x)\n",
    "print std_x.shape\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "std_normalizer = Normalizer().fit(std_x)\n",
    "std_x = std_normalizer.transform(std_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD-cv10-LR, mse: [ 0.06500286  0.03528658  0.1532927   0.01377453  0.08483742  0.04885342\n",
      "  0.07232473  0.04269288  0.02168829  0.0902642 ]\n",
      "STD-cv10-LR, mse mean: 0.0628017604365\n",
      "STD-cv10-LR, rmse: [ 0.25495658  0.18784721  0.39152611  0.11736496  0.29126863  0.22102809\n",
      "  0.26893258  0.20662256  0.14726944  0.30044002]\n",
      "STD-cv10-LR, rmse mean: 0.238725618604\n"
     ]
    }
   ],
   "source": [
    "#std-cv10-lr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "std_lr = LinearRegression()\n",
    "\n",
    "std_scores = cross_val_score(std_lr, std_x, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'STD-cv10-LR, mse:', std_mse\n",
    "print 'STD-cv10-LR, mse mean:', np.mean(std_mse)\n",
    "print 'STD-cv10-LR, rmse:', np.sqrt(std_mse)\n",
    "print 'STD-cv10-LR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD-cv10-LSVR, mse: [ 0.11377503  0.03723815  0.17545945  0.00339732  0.11292987  0.08042356\n",
      "  0.09290938  0.05865561  0.00655683  0.12968777]\n",
      "STD-cv10-LSVR, mse mean: 0.0811032975455\n",
      "STD-cv10-LSVR, rmse: [ 0.33730554  0.19297188  0.41887881  0.05828654  0.3360504   0.28359048\n",
      "  0.30481039  0.24218921  0.08097424  0.36012189]\n",
      "STD-cv10-LSVR, rmse mean: 0.261517938491\n"
     ]
    }
   ],
   "source": [
    "#std-cv10-lsvr\n",
    "from sklearn.svm import LinearSVR\n",
    "std_lsvr = LinearSVR()\n",
    "\n",
    "std_scores = cross_val_score(std_lsvr, std_x, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'STD-cv10-LSVR, mse:', std_mse\n",
    "print 'STD-cv10-LSVR, mse mean:', np.mean(std_mse)\n",
    "print 'STD-cv10-LSVR, rmse:', np.sqrt(std_mse)\n",
    "print 'STD-cv10-LSVR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD-cv10-SVR, mse: [ 0.10395625  0.03702948  0.15147344  0.01011492  0.09645771  0.0735336\n",
      "  0.07717419  0.0560485   0.01123691  0.11054614]\n",
      "STD-cv10-SVR, mse mean: 0.0727571150049\n",
      "STD-cv10-SVR, rmse: [ 0.32242247  0.19243046  0.38919589  0.10057296  0.31057642  0.27117079\n",
      "  0.27780244  0.23674563  0.10600431  0.3324848 ]\n",
      "STD-cv10-SVR, rmse mean: 0.253940618231\n",
      "STD-svr cost: 0.0290849208832\n"
     ]
    }
   ],
   "source": [
    "#std-cv10-svr\n",
    "t0 = time()\n",
    "from sklearn.svm import SVR\n",
    "std_svr = SVR()\n",
    "\n",
    "std_scores = cross_val_score(std_svr, std_x, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'STD-cv10-SVR, mse:', std_mse\n",
    "print 'STD-cv10-SVR, mse mean:', np.mean(std_mse)\n",
    "print 'STD-cv10-SVR, rmse:', np.sqrt(std_mse)\n",
    "print 'STD-cv10-SVR, rmse mean:', np.mean(np.sqrt(std_mse))\n",
    "print \"STD-svr cost:\", (time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def cut sentence\n",
    "stop_words = 'a,able,about,across,after,all,almost,also,am,among,an,and,any,are,as,at,be,because,been,but,by,can,cannot,could,dear,did,do,does,either,else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,me,might,most,must,my,neither,no,nor,not,of,off,often,on,only,or,other,our,own,rather,said,say,says,she,should,since,so,some,than,that,the,their,them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,what,when,where,which,while,who,whom,why,will,with,would,yet,you,your'\n",
    "stop_words_list = stop_words.split(',')\n",
    "\n",
    "def cut_into_tokens(text):\n",
    "\n",
    "    _list = [word.lower() for sentence in nltk.sent_tokenize(text) for word in sentence.split()]\n",
    "    _list = [re.sub(r'[.?!\\'\",)(;&%:]', '', word) for word in _list  ]\n",
    "    _list = [word for word in _list if word not in stop_words_list and len(word) > 0]\n",
    "    return _list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic features of Kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " If you're reading this, then you've seen this movie or are at least curious what all the hype is about?    The late Stanley Kubrick, the only major filmmaker Lynch has cited as a direct cinematic influence, believed that ERASERHEAD was one of the most perfect \"cinematic experiences\" created to date.  This movie has enjoyed success on the midnight movie circuit for years, particularly in NYC where it ran almost every night for something like five years straight. I've seen it on big and little screens in three different states.  Insofar as interpretations are concerned, I've long since tossed all that out the window.  In terms of rational comprehension, ERASERHEAD is the fabled big fish that remains brilliantly elusive of any attempts to capture it.     This movie gets better, and more humorous, every time I watch it: in my opinion - ERASERHEAD is the cinematic experience that comes the closest to capturing \"dream logic\", next to the equally brilliant WAKING LIFE.  If you ever get the chance, watch ERASERHEAD in a movie theater with a great sound system - you will understand why Stanley Kubrick was moved enough to make his statement.  It's like experiencing someone else's dream - the ultimate act of voyeurism?  As if I was granted audience to a demonstration of delicate brain surgery, and catching glimpses of the patient's face throughout the operation (particularly the opening scene).  It creates such a visceral landscape with its dark, peculiar selections of image and sound, that it seems to be constantly reminding you that the \"soul\" is helplessly sloshing around somewhere inside an organic bag of blood, bone, hair follicles, industrial shrapnel, dirt piles and antique radiators; a terrifying and beautiful delineation of a living creature suddenly made aware of its own being (birth imagery abounding).  It is a perfect symphony of sound and image, amazing work for a first time feature film director!  I've seen this movie placed in the HORROR section at local video stores; it's better suited for the COMEDY section, I fear.  The movie was created on the AFI campus in California; production beginning his last year there, and continuing on for several more years in secret.  Not for everyone, but certainly worth a peek.    I own a copy of ERASERHEAD on DVD, finally.  It is available at David Lynch's website for forty-five bucks plus shipping and handling.  Remastered sound and image, includes a few extras - the standout is a \"stylized\" interview with Lynch about the making of the film, the characters involved and anecdotes.    BTW - where is WILD AT HEART and LOST HIGHWAY for our DVD pleasure?     \n",
      "\n",
      "[115, 50, 42, 21]\n",
      "[13, 5, 6, 4]\n",
      "[61, 30, 31, 10]\n",
      "[49, 20, 11, 9]\n",
      "[72, 61, 21, 14]\n",
      "[51, 22, 15, 7]\n",
      "[131, 68, 38, 18]\n",
      "[9, 5, 6, 5]\n",
      "[30, 17, 14, 11]\n",
      "[62, 31, 10, 11]\n",
      "syn_exc is  0\n"
     ]
    }
   ],
   "source": [
    "#Syntactic features of Kim\n",
    "\n",
    "text = df.body.head(1)[0]\n",
    "print text\n",
    "#print nltk.pos_tag([word  for sentence in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sentence)])\n",
    "\n",
    "def pos_count(text):\n",
    "    poses = nltk.pos_tag(cut_into_tokens(text))\n",
    "#     poses = nltk.pos_tag([word  for sentence in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sentence)])\n",
    "    pos_dict = {}\n",
    "    for (word, pos) in poses:\n",
    "        if pos in pos_dict:\n",
    "            pos_dict[pos] = pos_dict[pos] + 1\n",
    "        else:\n",
    "            pos_dict[pos] = 1\n",
    "    return pos_dict\n",
    "\n",
    "pos_dict = pos_count(text)#per sentence per second!!!!\n",
    "# for pos in pos_dict:\n",
    "#     print pos, \" => \", pos_dict[pos]\n",
    "# print 'aa'\n",
    "syn_exc = 0\n",
    "\n",
    "def fetch_count_from_pos_dict(pos_list, pos_dict):\n",
    "    _pos_count = 0\n",
    "    for pos in pos_list:\n",
    "        if pos in pos_dict:\n",
    "            _pos_count += pos_dict[pos]\n",
    "    return _pos_count\n",
    "\n",
    "def syntactic_feature(text): #syn feature function\n",
    "    ary = [0, 0, 0, 0]\n",
    "    try:\n",
    "        pos_dict = pos_count(text)\n",
    "        nn_count = fetch_count_from_pos_dict(['NN', 'NNS', 'NNP', 'NNPS'], pos_dict)#noun\n",
    "        jj_count = fetch_count_from_pos_dict(['JJ', 'JJR', 'JJS'], pos_dict)#adj\n",
    "        vb_count = fetch_count_from_pos_dict(['VB', 'VBD', 'VBG', 'VBN', 'VNP', 'VBZ'], pos_dict)#vb\n",
    "        rb_count = fetch_count_from_pos_dict(['RB', 'RBR', 'RBS'], pos_dict)#adv\n",
    "        ary = [nn_count, jj_count, vb_count, rb_count]\n",
    "    except:\n",
    "        print 'error'\n",
    "        syn_exc += 1\n",
    "    finally:\n",
    "        return ary\n",
    "# print 't_dict', pos_dict['_t_dict']\n",
    "for _text in df.body.head(10):\n",
    "    print syntactic_feature(_text)\n",
    "print 'syn_exc is ', syn_exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Normalizer(copy=True, norm='l2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X_syn = []\n",
    "for _body in df.body:\n",
    "    _v = syntactic_feature(_body)\n",
    "    X_syn.append(_v)\n",
    "print len(X_syn)\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "syn_normalizer = Normalizer()\n",
    "syn_normalizer.fit(X_syn)\n",
    "X_syn = syn_normalizer.transform(X_syn)\n",
    "\n",
    "# print X_w2v\n",
    "print syn_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYN-cv10-LR, mse: [ 0.10462254  0.05454791  0.13415301  0.03058137  0.11306078  0.08782886\n",
      "  0.05758035  0.0522465   0.01209688  0.07854349]\n",
      "SYN-cv10-LR, mse mean: 0.0725261692847\n",
      "SYN-cv10-LR, rmse: [ 0.32345407  0.23355494  0.36626905  0.1748753   0.33624512  0.29635935\n",
      "  0.23995905  0.22857494  0.1099858   0.28025611]\n",
      "SYN-cv10-LR, rmse mean: 0.258953373802\n"
     ]
    }
   ],
   "source": [
    "#syn-cv-10-LR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "syn_lr = LinearRegression()\n",
    "\n",
    "std_scores = cross_val_score(syn_lr, X_syn, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'SYN-cv10-LR, mse:', std_mse\n",
    "print 'SYN-cv10-LR, mse mean:', np.mean(std_mse)\n",
    "print 'SYN-cv10-LR, rmse:', np.sqrt(std_mse)\n",
    "print 'SYN-cv10-LR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYN-cv10-LSVR, mse: [ 0.13102097  0.046614    0.17911154  0.00585943  0.11172521  0.09141683\n",
      "  0.09245582  0.05799587  0.00256428  0.12499173]\n",
      "SYN-cv10-LSVR, mse mean: 0.0843755699511\n",
      "SYN-cv10-LSVR, rmse: [ 0.36196818  0.21590277  0.42321572  0.07654695  0.33425322  0.30235217\n",
      "  0.30406548  0.24082331  0.05063877  0.3535417 ]\n",
      "SYN-cv10-LSVR, rmse mean: 0.266330826192\n"
     ]
    }
   ],
   "source": [
    "#syn-cv-10-LSVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "syn_lsvr = LinearSVR()\n",
    "\n",
    "std_scores = cross_val_score(syn_lsvr, X_syn, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'SYN-cv10-LSVR, mse:', std_mse\n",
    "print 'SYN-cv10-LSVR, mse mean:', np.mean(std_mse)\n",
    "print 'SYN-cv10-LSVR, rmse:', np.sqrt(std_mse)\n",
    "print 'SYN-cv10-LSVR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYN-cv10-SVR, mse: [ 0.10835548  0.04106706  0.15071772  0.0124428   0.09822887  0.0789925\n",
      "  0.07749351  0.05689127  0.00612068  0.10719769]\n",
      "SYN-cv10-SVR, mse mean: 0.0737507600257\n",
      "SYN-cv10-SVR, rmse: [ 0.32917394  0.20265009  0.38822381  0.11154731  0.31341485  0.28105605\n",
      "  0.27837656  0.23851892  0.07823481  0.32741059]\n",
      "SYN-cv10-SVR, rmse mean: 0.254860691454\n",
      "SYN-svr cost: 0.0341169834137\n"
     ]
    }
   ],
   "source": [
    "#syn-cv-10-SVR\n",
    "t0 = time()\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "syn_lsvr = SVR()\n",
    "\n",
    "std_scores = cross_val_score(syn_lsvr, X_syn, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'SYN-cv10-SVR, mse:', std_mse\n",
    "print 'SYN-cv10-SVR, mse mean:', np.mean(std_mse)\n",
    "print 'SYN-cv10-SVR, rmse:', np.sqrt(std_mse)\n",
    "print 'SYN-cv10-SVR, rmse mean:', np.mean(np.sqrt(std_mse))\n",
    "print \"SYN-svr cost:\", (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#w2v features\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "\n",
    "#load model\n",
    "fname = '/data/opinion_spam/real/w2v-hfd-over-0.total'\n",
    "#gensim.models.Word2Vec(sentences, min_count=5, size=100, window=5, workers=32)\n",
    "model = gensim.models.Word2Vec.load(fname)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model_with_sentence\n"
     ]
    }
   ],
   "source": [
    "def model_with_sentence(sentence, model):\n",
    "    tokens = cut_into_tokens(sentence)\n",
    "    v = np.zeros(100)\n",
    "    _error = 0\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            _v = model[token]\n",
    "            v += _v\n",
    "        except:\n",
    "            _error += 1\n",
    "#             print 'token:', token, 'not in the dict'\n",
    "    return v\n",
    "print 'def model_with_sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Normalizer(copy=True, norm='l2')\n"
     ]
    }
   ],
   "source": [
    "X_w2v = []\n",
    "for _body in df.body:\n",
    "    _v = model_with_sentence(_body, model)\n",
    "    X_w2v.append(_v)\n",
    "print len(X_w2v)\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "w2v_normalizer = Normalizer()\n",
    "w2v_normalizer.fit(X_w2v)\n",
    "X_w2v = w2v_normalizer.transform(X_w2v)\n",
    "\n",
    "# print X_w2v\n",
    "print w2v_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V-cv10-LR, mse: [ 0.23995108  0.4017458   0.90328586  0.33569364  0.39553965  0.31661745\n",
      "  0.39600169  0.3550751   0.14575424  0.32750875]\n",
      "W2V-cv10-LR, mse mean: 0.381717325347\n",
      "W2V-cv10-LR, rmse: [ 0.48984801  0.6338342   0.95041352  0.57939075  0.62891943  0.5626877\n",
      "  0.62928665  0.59588179  0.38177774  0.5722838 ]\n",
      "W2V-cv10-LR, rmse mean: 0.602432359351\n"
     ]
    }
   ],
   "source": [
    "#w2v-cv-10-LR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "w2v_lr = LinearRegression()\n",
    "\n",
    "std_scores = cross_val_score(w2v_lr, X_w2v, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'W2V-cv10-LR, mse:', std_mse\n",
    "print 'W2V-cv10-LR, mse mean:', np.mean(std_mse)\n",
    "print 'W2V-cv10-LR, rmse:', np.sqrt(std_mse)\n",
    "print 'W2V-cv10-LR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V-cv10-LSVR, mse: [ 0.03931486  0.03252135  0.09752162  0.02300118  0.03382264  0.03624022\n",
      "  0.05279269  0.03912735  0.01959384  0.09781817]\n",
      "W2V-cv10-LSVR, mse mean: 0.0471753910219\n",
      "W2V-cv10-LSVR, rmse: [ 0.19827976  0.18033678  0.31228451  0.15166139  0.18390933  0.19036863\n",
      "  0.22976659  0.19780635  0.13997798  0.31275896]\n",
      "W2V-cv10-LSVR, rmse mean: 0.209715028807\n"
     ]
    }
   ],
   "source": [
    "#w2v-cv-10-LSVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "w2v_lsvr = LinearSVR()\n",
    "\n",
    "std_scores = cross_val_score(w2v_lsvr, X_w2v, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'W2V-cv10-LSVR, mse:', std_mse\n",
    "print 'W2V-cv10-LSVR, mse mean:', np.mean(std_mse)\n",
    "print 'W2V-cv10-LSVR, rmse:', np.sqrt(std_mse)\n",
    "print 'W2V-cv10-LSVR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V-cv10-SVR, mse: [ 0.08787033  0.0349269   0.12780129  0.01049571  0.07806817  0.06075669\n",
      "  0.05939609  0.04728418  0.00699689  0.08921855]\n",
      "W2V-cv10-SVR, mse mean: 0.0602814795843\n",
      "W2V-cv10-SVR, rmse: [ 0.2964293   0.18688739  0.35749306  0.10244857  0.27940682  0.24648872\n",
      "  0.24371313  0.21744927  0.08364741  0.29869474]\n",
      "W2V-cv10-SVR, rmse mean: 0.231265840941\n",
      "W2V-svr cost: 0.0333380699158\n"
     ]
    }
   ],
   "source": [
    "#w2v-cv-10-SVR\n",
    "t0 = time()\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "w2v_svr = SVR()\n",
    "\n",
    "std_scores = cross_val_score(w2v_svr, X_w2v, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'W2V-cv10-SVR, mse:', std_mse\n",
    "print 'W2V-cv10-SVR, mse mean:', np.mean(std_mse)\n",
    "print 'W2V-cv10-SVR, rmse:', np.sqrt(std_mse)\n",
    "print 'W2V-cv10-SVR, rmse mean:', np.mean(np.sqrt(std_mse))\n",
    "print \"W2V-svr cost:\", (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STD + Word2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size :  105\n",
      "Normalizer(copy=True, norm='l2')\n"
     ]
    }
   ],
   "source": [
    "#combine std + w2v\n",
    "X_std_w2v = []\n",
    "for _body in df.body:\n",
    "    _w2v = model_with_sentence(_body, model)\n",
    "    _std = std(_body)\n",
    "    _v = np.append(_w2v, _std)\n",
    "    X_std_w2v.append(_v)\n",
    "print 'feature size : ', len(X_std_w2v[0])\n",
    "\n",
    "std_w2v_normalizer = Normalizer()\n",
    "std_w2v_normalizer.fit(X_w2v)\n",
    "X_std_w2v = std_w2v_normalizer.transform(X_w2v)\n",
    "\n",
    "# print X_w2v\n",
    "print std_w2v_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD+W2V-cv10-LR, mse: [ 0.18428478  0.4017458   0.90328586  0.33066218  0.39553965  0.32222483\n",
      "  0.37078414  0.3550751   0.18085962  0.32750875]\n",
      "STD+W2V-cv10-LR, mse mean: 0.377197069472\n",
      "STD+W2V-cv10-LR, rmse: [ 0.42928404  0.6338342   0.95041352  0.57503233  0.62891943  0.56764851\n",
      "  0.60892047  0.59588179  0.42527593  0.5722838 ]\n",
      "STD+W2V-cv10-LR, rmse mean: 0.598749400454\n"
     ]
    }
   ],
   "source": [
    "#std+w2v-cv-10-LR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "std_w2v_lr = LinearRegression()\n",
    "\n",
    "std_scores = cross_val_score(std_w2v_lr, X_std_w2v, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'STD+W2V-cv10-LR, mse:', std_mse\n",
    "print 'STD+W2V-cv10-LR, mse mean:', np.mean(std_mse)\n",
    "print 'STD+W2V-cv10-LR, rmse:', np.sqrt(std_mse)\n",
    "print 'STD+W2V-cv10-LR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD+W2V-cv10-LSVR, mse: [ 0.03930491  0.03251312  0.09751806  0.02300316  0.03380921  0.036244\n",
      "  0.05278769  0.03913686  0.0196003   0.0978142 ]\n",
      "STD+W2V-cv10-LSVR, mse mean: 0.0471731488588\n",
      "STD+W2V-cv10-LSVR, rmse: [ 0.19825465  0.18031395  0.31227881  0.15166792  0.1838728   0.19037856\n",
      "  0.22975572  0.19783037  0.14000106  0.31275262]\n",
      "STD+W2V-cv10-LSVR, rmse mean: 0.209710646216\n"
     ]
    }
   ],
   "source": [
    "#std+w2v-cv-10-LSVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "std_w2v_lsvr = LinearSVR()\n",
    "\n",
    "std_scores = cross_val_score(std_w2v_lsvr, X_std_w2v, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'STD+W2V-cv10-LSVR, mse:', std_mse\n",
    "print 'STD+W2V-cv10-LSVR, mse mean:', np.mean(std_mse)\n",
    "print 'STD+W2V-cv10-LSVR, rmse:', np.sqrt(std_mse)\n",
    "print 'STD+W2V-cv10-LSVR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD+W2V-cv10-SVR, mse: [ 0.08787033  0.0349269   0.12780129  0.01049571  0.07806817  0.06075669\n",
      "  0.05939609  0.04728418  0.00699689  0.08921855]\n",
      "STD+W2V-cv10-SVR, mse mean: 0.0602814795843\n",
      "STD+W2V-cv10-SVR, rmse: [ 0.2964293   0.18688739  0.35749306  0.10244857  0.27940682  0.24648872\n",
      "  0.24371313  0.21744927  0.08364741  0.29869474]\n",
      "STD+W2V-cv10-SVR, rmse mean: 0.231265840941\n",
      "STD+W2V-svr cost: 0.0328810214996\n"
     ]
    }
   ],
   "source": [
    "#std+w2v-cv-10-SVR\n",
    "t0 = time()\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "std_w2v_svr = SVR()\n",
    "\n",
    "std_scores = cross_val_score(std_w2v_svr, X_std_w2v, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'STD+W2V-cv10-SVR, mse:', std_mse\n",
    "print 'STD+W2V-cv10-SVR, mse mean:', np.mean(std_mse)\n",
    "print 'STD+W2V-cv10-SVR, rmse:', np.sqrt(std_mse)\n",
    "print 'STD+W2V-cv10-SVR, rmse mean:', np.mean(np.sqrt(std_mse))\n",
    "print \"STD+W2V-svr cost:\", (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# STD + SYN features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size :  9\n",
      "Normalizer(copy=True, norm='l2')\n"
     ]
    }
   ],
   "source": [
    "X_std_syn = []\n",
    "for _body in df.body:\n",
    "    _std = std(_body)\n",
    "    _syn = syntactic_feature(_body)\n",
    "    _v = np.append(_std, _syn)\n",
    "    X_std_syn.append(_v)\n",
    "print 'feature size : ', len(X_std_syn[0])\n",
    "\n",
    "std_syn_normalizer = Normalizer()\n",
    "std_syn_normalizer.fit(X_std_syn)\n",
    "X_std_syn = std_syn_normalizer.transform(X_std_syn)\n",
    "\n",
    "# print X_w2v\n",
    "print std_syn_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD+SYN-cv10-LR, mse: [ 0.06594952  0.03012259  0.16264065  0.04461     0.08793791  0.05437565\n",
      "  0.06926479  0.03886259  0.01363866  0.08043925]\n",
      "STD+SYN-cv10-LR, mse mean: 0.0647841613625\n",
      "STD+SYN-cv10-LR, rmse: [ 0.25680638  0.17355862  0.40328731  0.2112108   0.29654327  0.23318587\n",
      "  0.26318204  0.19713598  0.11678466  0.28361814]\n",
      "STD+SYN-cv10-LR, rmse mean: 0.243531306594\n"
     ]
    }
   ],
   "source": [
    "#std+syn-cv-10-LR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "std_syn_lr = LinearRegression()\n",
    "\n",
    "std_scores = cross_val_score(std_syn_lr, X_std_syn, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'STD+SYN-cv10-LR, mse:', std_mse\n",
    "print 'STD+SYN-cv10-LR, mse mean:', np.mean(std_mse)\n",
    "print 'STD+SYN-cv10-LR, rmse:', np.sqrt(std_mse)\n",
    "print 'STD+SYN-cv10-LR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD+SYN-cv10-LSVR, mse: [ 0.11724994  0.03955369  0.18195911  0.00437252  0.11164453  0.08057741\n",
      "  0.09146775  0.06207023  0.00743744  0.12674247]\n",
      "STD+SYN-cv10-LSVR, mse mean: 0.0823075082408\n",
      "STD+SYN-cv10-LSVR, rmse: [ 0.34241779  0.19888109  0.42656665  0.066125    0.3341325   0.2838616\n",
      "  0.30243635  0.24913898  0.08624062  0.35600909]\n",
      "STD+SYN-cv10-LSVR, rmse mean: 0.264580966907\n"
     ]
    }
   ],
   "source": [
    "#std+syn-cv-10-LSVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "std_syn_lsvr = LinearSVR()\n",
    "\n",
    "std_scores = cross_val_score(std_syn_lsvr, X_std_syn, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'STD+SYN-cv10-LSVR, mse:', std_mse\n",
    "print 'STD+SYN-cv10-LSVR, mse mean:', np.mean(std_mse)\n",
    "print 'STD+SYN-cv10-LSVR, rmse:', np.sqrt(std_mse)\n",
    "print 'STD+SYN-cv10-LSVR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD+SYN-cv10-SVR, mse: [ 0.10308946  0.03709436  0.15505091  0.01060413  0.09383702  0.069359\n",
      "  0.07583103  0.05522573  0.00994273  0.10665596]\n",
      "STD+SYN-cv10-SVR, mse mean: 0.0716690335944\n",
      "STD+SYN-cv10-SVR, rmse: [ 0.32107548  0.19259895  0.39376504  0.10297636  0.30632829  0.26336098\n",
      "  0.27537435  0.23500155  0.09971324  0.32658225]\n",
      "STD+SYN-cv10-SVR, rmse mean: 0.251677648098\n"
     ]
    }
   ],
   "source": [
    "#std+syn-cv-10-SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "std_syn_svr = SVR()\n",
    "\n",
    "std_scores = cross_val_score(std_syn_svr, X_std_syn, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'STD+SYN-cv10-SVR, mse:', std_mse\n",
    "print 'STD+SYN-cv10-SVR, mse mean:', np.mean(std_mse)\n",
    "print 'STD+SYN-cv10-SVR, rmse:', np.sqrt(std_mse)\n",
    "print 'STD+SYN-cv10-SVR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STD + SYN + W2V features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size :  109\n",
      "Normalizer(copy=True, norm='l2')\n"
     ]
    }
   ],
   "source": [
    "X_std_syn_w2v = []\n",
    "for _body in df.body:\n",
    "    _std = std(_body)\n",
    "    _syn = syntactic_feature(_body)\n",
    "    _w2v = model_with_sentence(_body, model)\n",
    "    _v = np.append(_std, _syn)\n",
    "    _v = np.append(_v, _w2v)\n",
    "    X_std_syn_w2v.append(_v)\n",
    "print 'feature size : ', len(X_std_syn_w2v[0])\n",
    "\n",
    "std_syn_w2v_normalizer = Normalizer()\n",
    "std_syn_w2v_normalizer.fit(X_std_syn)\n",
    "X_std_syn_w2v = std_syn_w2v_normalizer.transform(X_std_syn)\n",
    "\n",
    "# print X_w2v\n",
    "print std_syn_w2v_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD+SYN+W2V-cv10-LR, mse: [ 0.06594952  0.03012259  0.16264065  0.04461     0.08793791  0.05437565\n",
      "  0.06926479  0.03886259  0.01363866  0.08043925]\n",
      "STD+SYN+W2V-cv10-LR, mse mean: 0.0647841613625\n",
      "STD+SYN+W2V-cv10-LR, rmse: [ 0.25680638  0.17355862  0.40328731  0.2112108   0.29654327  0.23318587\n",
      "  0.26318204  0.19713598  0.11678466  0.28361814]\n",
      "STD+SYN+W2V-cv10-LR, rmse mean: 0.243531306594\n"
     ]
    }
   ],
   "source": [
    "#std+syn+w2v-cv-10-LR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "std_syn_w2v_lr = LinearRegression()\n",
    "\n",
    "std_scores = cross_val_score(std_syn_w2v_lr, X_std_syn_w2v, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'STD+SYN+W2V-cv10-LR, mse:', std_mse\n",
    "print 'STD+SYN+W2V-cv10-LR, mse mean:', np.mean(std_mse)\n",
    "print 'STD+SYN+W2V-cv10-LR, rmse:', np.sqrt(std_mse)\n",
    "print 'STD+SYN+W2V-cv10-LR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD+SYN+W2V-cv10-LSVR, mse: [ 0.06594952  0.03012259  0.16264065  0.04461     0.08793791  0.05437565\n",
      "  0.06926479  0.03886259  0.01363866  0.08043925]\n",
      "STD+SYN+W2V-cv10-LSVR, mse mean: 0.0647841613625\n",
      "STD+SYN+W2V-cv10-LSVR, rmse: [ 0.25680638  0.17355862  0.40328731  0.2112108   0.29654327  0.23318587\n",
      "  0.26318204  0.19713598  0.11678466  0.28361814]\n",
      "STD+SYN+W2V-cv10-LSVR, rmse mean: 0.243531306594\n"
     ]
    }
   ],
   "source": [
    "#std+syn+w2v-cv-10-LSVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "std_syn_w2v_lsvr = LinearRegression()\n",
    "\n",
    "std_scores = cross_val_score(std_syn_w2v_lsvr, X_std_syn_w2v, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'STD+SYN+W2V-cv10-LSVR, mse:', std_mse\n",
    "print 'STD+SYN+W2V-cv10-LSVR, mse mean:', np.mean(std_mse)\n",
    "print 'STD+SYN+W2V-cv10-LSVR, rmse:', np.sqrt(std_mse)\n",
    "print 'STD+SYN+W2V-cv10-LSVR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD+SYN+W2V-cv10-SVR, mse: [ 0.10308946  0.03709436  0.15505091  0.01060413  0.09383702  0.069359\n",
      "  0.07583103  0.05522573  0.00994273  0.10665596]\n",
      "STD+SYN+W2V-cv10-SVR, mse mean: 0.0716690335944\n",
      "STD+SYN+W2V-cv10-SVR, rmse: [ 0.32107548  0.19259895  0.39376504  0.10297636  0.30632829  0.26336098\n",
      "  0.27537435  0.23500155  0.09971324  0.32658225]\n",
      "STD+SYN+W2V-cv10-SVR, rmse mean: 0.251677648098\n"
     ]
    }
   ],
   "source": [
    "#std+syn+w2v-cv-10-LSVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "std_syn_w2v_svr = SVR()\n",
    "\n",
    "std_scores = cross_val_score(std_syn_w2v_svr, X_std_syn_w2v, df.hfdfd, cv=10, scoring='mean_squared_error')\n",
    "\n",
    "std_mse = abs(std_scores)\n",
    "print 'STD+SYN+W2V-cv10-SVR, mse:', std_mse\n",
    "print 'STD+SYN+W2V-cv10-SVR, mse mean:', np.mean(std_mse)\n",
    "print 'STD+SYN+W2V-cv10-SVR, rmse:', np.sqrt(std_mse)\n",
    "print 'STD+SYN+W2V-cv10-SVR, rmse mean:', np.mean(np.sqrt(std_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TFIDF + STD features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_to_array(_matrix):\n",
    "    _dense = _matrix.todense()\n",
    "    _list = _dense.tolist()\n",
    "    return _list\n",
    "\n",
    "def append_matrix_to_matrix(array1, array2):\n",
    "    length = len(array1)#row num\n",
    "    new_matrix = []\n",
    "    for index in range(length):\n",
    "        _row = np.append(array1[index], array2[index])\n",
    "        new_matrix.append(_row)\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 3, 5, 6, 7]), array([2, 3, 4, 7, 8, 9])]\n"
     ]
    }
   ],
   "source": [
    "#validate the matrix merge\n",
    "ar1 = [[1,2,3],[2,3,4]]\n",
    "ar2 = [[5,6,7], [7,8,9]]\n",
    "ar3 = append_matrix_to_matrix(ar1, ar2)\n",
    "print ar3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf list:  100\n",
      "type v0:  <type 'list'>\n",
      "list 0: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07128882996115371, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07988332120543765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045611980844259714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07315959415070479, 0.0, 0.0, 0.06955677285695273, 0.0, 0.0, 0.0, 0.0, 0.07128882996115371, 0.0, 0.0, 0.08935988223651822, 0.0, 0.0, 0.0, 0.0, 0.08935988223651822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07128882996115371, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062419369953786476, 0.0, 0.0, 0.0, 0.06643586709597193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05600521133578724, 0.0, 0.0, 0.08575706094276617, 0.0, 0.0, 0.0, 0.0988364432675988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07742082730169302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.08935988223651822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0988364432675988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06643586709597193, 0.0, 0.0, 0.08935988223651822, 0.0, 0.0, 0.0, 0.0751932306041296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052536327802752386, 0.0988364432675988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08575706094276617, 0.0, 0.0, 0.0, 0.07742082730169302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08935988223651822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07988332120543765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07742082730169302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07315959415070479, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09362111538750645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08263615518178537, 0.0, 0.08575706094276617, 0.0, 0.08575706094276617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09362111538750645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05899294251831616, 0.0, 0.0, 0.0, 0.0, 0.07128882996115371, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0988364432675988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06794426627061244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08935988223651822, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06008021182587216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08263615518178537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062419369953786476, 0.0, 0.0, 0.07315959415070479, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06955677285695273, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07988332120543765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07742082730169302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08935988223651822, 0.06368303311962421, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08575706094276617, 0.10556017032233166, 0.07128882996115371, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0988364432675988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0988364432675988, 0.0, 0.044443133529685465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.08935988223651822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0751932306041296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0988364432675988, 0.0, 0.0, 0.09362111538750645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08935988223651822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.048141156891046956, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08935988223651822, 0.0, 0.0, 0.08935988223651822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08935988223651822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08935988223651822, 0.09362111538750645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09362111538750645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09362111538750645, 0.0, 0.0, 0.0, 0.0988364432675988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06122053921587959, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05695930606489136, 0.06643586709597193, 0.0, 0.0, 0.0, 0.0988364432675988, 0.0, 0.08263615518178537, 0.0, 0.0, 0.06643586709597193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08263615518178537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07315959415070479, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0751932306041296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07988332120543765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04279265443250273, 0.0, 0.07988332120543765, 0.08263615518178537, 0.08575706094276617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0751932306041296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07742082730169302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0988364432675988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0751932306041296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06368303311962421, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0988364432675988, 0.0, 0.0, 0.0, 0.0, 0.08575706094276617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06955677285695273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0988364432675988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.040277102446306674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0988364432675988, 0.0, 0.0, 0.08935988223651822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07988332120543765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08935988223651822, 0.08575706094276617, 0.0, 0.0, 0.07742082730169302, 0.0, 0.0751932306041296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07128882996115371, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05795401257707635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08575706094276617, 0.062419369953786476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0988364432675988, 0.06368303311962421, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.06368303311962421, 0.09362111538750645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08263615518178537, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07988332120543765, 0.0988364432675988, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07742082730169302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08935988223651822, 0.0, 0.06368303311962421, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062419369953786476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08575706094276617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06955677285695273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10556017032233166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062419369953786476, 0.0, 0.0, 0.0, 0.0, 0.08935988223651822, 0.0, 0.0, 0.0988364432675988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.048141156891046956, 0.0, 0.0, 0.0, 0.0, 0.07988332120543765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04881865578509925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "list 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24500749714418496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25280035455887306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2713885087038977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2827900689636479, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25280035455887306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33405782436301584, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2102442725696868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24500749714418496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31277978336842277, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23795800012202212, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13542248932328618, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26151202796905476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2315223135642799, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12746172324063673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2102442725696868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22012075330452974, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19753358545368754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "1856\n"
     ]
    }
   ],
   "source": [
    "tfidf_list = matrix_to_array(tfidf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "#print tfidf_list\n",
    "\n",
    "\n",
    "print 'tfidf list: ',len(tfidf_list)\n",
    "\n",
    "v0 = tfidf_list[0]\n",
    "\n",
    "print 'type v0: ', type(v0)\n",
    "\n",
    "print 'list 0:',v0\n",
    "\n",
    "\n",
    "print 'list 1:',tfidf_list[1]\n",
    "print len(tfidf_list[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
