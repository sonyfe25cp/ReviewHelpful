{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                      int64\n",
      "title                  object\n",
      "content                object\n",
      "publishDate    datetime64[ns]\n",
      "dtype: object\n",
      "(2000, 4)\n"
     ]
    }
   ],
   "source": [
    "#read from mysql\n",
    "import MySQLdb\n",
    "conn= MySQLdb.connect(\n",
    "        host='192.168.1.101',\n",
    "        port = 3310,\n",
    "        user='root',\n",
    "        passwd='webrisk',\n",
    "        db ='webrisk',\n",
    "        charset=\"gbk\"\n",
    "        )\n",
    "# cur = conn.cursor()\n",
    "\n",
    "sql =\"select id, title, content, publishDate from articles order by id desc limit 500\"\n",
    "#total_num = cur.execute(sql);\n",
    "#print \"total: \", total_num\n",
    "#line = cur.fetchone()\n",
    "#id = line[0]\n",
    "#title = line[1]\n",
    "#body = line[2]\n",
    "#date = line[3]\n",
    "\n",
    "df = pd.read_sql_query(sql, conn, parse_dates=['publishDate'])\n",
    "print df.dtypes\n",
    "print df.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>publishDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168316</td>\n",
       "      <td>35家化妆品企业遭点名 俏十岁等知名微商品牌上榜</td>\n",
       "      <td>35家化妆品企业遭点名 部分知名微商品牌上榜 ◎每经记者 金喆 市场空间较为艰难的微商又有了...</td>\n",
       "      <td>2016-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168315</td>\n",
       "      <td>微软再遭工商总局反垄断调查 表态“将积极配合”</td>\n",
       "      <td>尽管已是2016年，但工商总局在2014年立案，针对微软公司涉嫌垄断行为的调查仍未终结。 1...</td>\n",
       "      <td>2016-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168314</td>\n",
       "      <td>在岸人民币开盘升值100多个基点</td>\n",
       "      <td>【财新网】（记者 李雨谦）1月11日，在中国银行间外汇交易中心开盘后，在岸人民币汇率（C...</td>\n",
       "      <td>2016-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168313</td>\n",
       "      <td>房企打响股价保卫战 碧桂园推58亿回购计划</td>\n",
       "      <td>面对股票市场的不淡定，部分资金实力强劲的龙头房企纷纷通过回购股权抵御“空军”袭击。 千亿房企...</td>\n",
       "      <td>2016-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168312</td>\n",
       "      <td>河北矿山一夜被盗10万吨 粉尘加重京津雾霾</td>\n",
       "      <td>自动播放开关 自动播放 直击河北三河东北矿区盗采现状 山体60米深坑 正在加载... &lt; &gt;...</td>\n",
       "      <td>2016-01-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                     title  \\\n",
       "0  168316  35家化妆品企业遭点名 俏十岁等知名微商品牌上榜   \n",
       "1  168315   微软再遭工商总局反垄断调查 表态“将积极配合”   \n",
       "2  168314          在岸人民币开盘升值100多个基点   \n",
       "3  168313     房企打响股价保卫战 碧桂园推58亿回购计划   \n",
       "4  168312     河北矿山一夜被盗10万吨 粉尘加重京津雾霾   \n",
       "\n",
       "                                             content publishDate  \n",
       "0  35家化妆品企业遭点名 部分知名微商品牌上榜 ◎每经记者 金喆 市场空间较为艰难的微商又有了...  2016-01-11  \n",
       "1  尽管已是2016年，但工商总局在2014年立案，针对微软公司涉嫌垄断行为的调查仍未终结。 1...  2016-01-11  \n",
       "2  　　【财新网】（记者 李雨谦）1月11日，在中国银行间外汇交易中心开盘后，在岸人民币汇率（C...  2016-01-11  \n",
       "3  面对股票市场的不淡定，部分资金实力强劲的龙头房企纷纷通过回购股权抵御“空军”袭击。 千亿房企...  2016-01-11  \n",
       "4  自动播放开关 自动播放 直击河北三河东北矿区盗采现状 山体60米深坑 正在加载... < >...  2016-01-11  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'generator'>\n",
      "我 在 马路边 捡 到 一分钱\n",
      "我\n",
      " \n",
      "在\n",
      " \n",
      "马\n",
      "路\n",
      "边\n",
      " \n",
      "捡\n",
      " \n",
      "到\n",
      " \n",
      "一\n",
      "分\n",
      "钱\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(u'我在马路边捡到一分钱', cut_all=False)\n",
    "print type(seg_list)\n",
    "print \" \".join(seg_list)\n",
    "\n",
    "def seg_chinese(text):\n",
    "    _seg_list = jieba.cut(text, cut_all=False)\n",
    "    return \" \".join(_seg_list)\n",
    "\n",
    "a = u'我在马路边捡到一分钱'\n",
    "for x in seg_chinese(a):\n",
    "    print x\n",
    "    \n",
    "\n",
    "\n",
    "#seg_list = jieba.cut(df.content.head(11)[0], cut_all=False)\n",
    "#print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df['words'] = df.content.apply(seg_chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3087)\n"
     ]
    }
   ],
   "source": [
    "#cluster\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "max_features_count = 10000\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=max_features_count,\n",
    "                                 min_df=2, tokenizer = seg_chinese)\n",
    "tfidf_matrix = vectorizer.fit_transform(df.content.head(1000))\n",
    "print tfidf_matrix.shape\n",
    "\n",
    "\n",
    "dbscan = DBSCAN(eps=0.7, min_samples=5).fit(tfidf_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN(algorithm='auto', eps=0.7, leaf_size=30, metric='euclidean',\n",
      "    min_samples=5, p=None, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "print dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label length: 1000\n",
      "labels\n",
      "-1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 1 -1 -1 -1 2 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 -1 -1 3 3 -1 -1 -1 -1 -1 -1 -1 4 -1 -1 -1 6 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 4 -1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 2 -1 -1 -1 -1 -1 1 -1 -1 -1 6 -1 -1 -1 -1 -1 0 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 9 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 5 5 -1 5 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 6 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 1 -1 -1 -1 7 4 0 -1 8 -1 -1 -1 8 -1 8 -1 -1 -1 -1 -1 -1 6 -1 -1 1 1 1 -1 1 0 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 6 -1 -1 10 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 9 -1 6 -1 -1 -1 -1 -1 -1 -1 -1 -1 10 3 -1 -1 0 2 -1 -1 -1 7 0 0 -1 0 -1 12 -1 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 3 -1 -1 3 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 6 -1 -1 -1 -1 -1 10 -1 -1 -1 -1 10 -1 -1 -1 -1 6 -1 -1 -1 -1 -1 10 -1 4 1 0 1 -1 -1 8 -1 -1 8 8 -1 -1 -1 -1 -1 -1 -1 8 -1 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 6 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 11 7 11 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 3 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 0 -1 -1 0 11 -1 -1 -1 -1 7 -1 -1 -1 -1 -1 11 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 9 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 11 -1 -1 11 -1 -1 -1 -1 -1 7 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 0 -1 -1 -1 0 -1 -1 0 11 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 0 -1 -1 -1 0 -1 0 -1 0 -1 -1 -1 -1 -1 0 -1 0 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 8 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 4 -1 -1 -1 -1 8 -1 0 -1 -1 -1 -1 0 -1 -1 -1 -1 4 -1 -1 -1 0 8 0 -1 -1 -1 -1 -1 -1 8 -1 -1 4 -1 0 -1 8 -1 -1 5 5 -1 -1 -1 -1 -1 5 5 9 -1 5 9 5 5 -1 -1 -1 -1 -1 -1 -1 9 -1 -1 -1 -1 -1 -1 -1 -1 -1 2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 12 9 -1 -1 12 -1 -1 -1 -1 -1 -1 9 -1 -1 -1 2 -1 -1 -1 -1 11 -1 -1 9 12 2 -1 -1 -1 -1 11 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 12 -1 8 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 8 -1 -1 -1 -1 11 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 11 11 -1 8 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n"
     ]
    }
   ],
   "source": [
    "labels = dbscan.labels_\n",
    "print 'label length:', len(labels)\n",
    "print 'labels'\n",
    "for _x in labels:\n",
    "    print _x,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster num: 13\n",
      "cluster size:  13\n",
      "168224 => 92 93 371 403 406 547\n",
      "167970 => 346 370 444 449 460\n",
      "168011 => 305 379 513 569 624\n",
      "168101 => 215 358 604 801 804 814 837 847 859\n",
      "168007 => 309 313 315 468 471 472 480 733 757 774 782 789 907 927 961\n",
      "168267 => 49 59 183 301 325 326 327 329 463 465 482\n",
      "168211 => 105 187 256 322 343 360 438 454 494\n",
      "168090 => 226 227 229 792 793 799 800 803 805 806\n",
      "168215 => 101 123 306 462 752 769 785 973\n",
      "168314 => 2 39 70 89 129 193 198 307 330 332 342 374 380 381 383 393 464 554 560 563 655 668 672 676 679 690 698 702 706 708 710 716 718 719 759 764 773 775 787 918\n",
      "167931 => 385 836 840 860 905\n",
      "167804 => 512 514 564 575 615 618 680 856 866 932 958 959\n",
      "168253 => 63 177 375 424 824 851 861\n",
      "(105, 3087)\n"
     ]
    }
   ],
   "source": [
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print 'Cluster num:', n_clusters_\n",
    "\n",
    "\n",
    "def find_result_dict(labels):#different group about labels and positions\n",
    "    _dict = {}\n",
    "    position = -1\n",
    "    for _x in labels:\n",
    "        position +=1\n",
    "        if _x in _dict:\n",
    "            _dict[_x] = _dict[_x] + \" \" + str(position)\n",
    "        else:\n",
    "            _dict[_x] = str(position)\n",
    "    _dict.pop(-1)\n",
    "    return _dict\n",
    "\n",
    "clu_dict = find_result_dict(labels)\n",
    "\n",
    "print 'cluster size: ', len(clu_dict)\n",
    "\n",
    "\n",
    "\n",
    "def find_cluster_dict(clu_dict):#get the maxRealId : articleId\n",
    "    _dict = {}\n",
    "    for _key in clu_dict:\n",
    "        _ids = [ int(_id) for _id in clu_dict[_key].split()]\n",
    "        _nids = [df.loc[_id].id for _id in _ids]\n",
    "        _nids_str = [str(_nid) for _nid in _nids]\n",
    "#         print \" \".join(_nids_str)\n",
    "        _dict[max(_nids)] = \" \".join(_nids_str)\n",
    "    return _dict\n",
    "\n",
    "def find_cluster_dict_dfId(clu_dict):#get the maxRealId : df_id\n",
    "    _dict = {}\n",
    "    for _key in clu_dict:\n",
    "        _ids = [ int(_id) for _id in clu_dict[_key].split()]\n",
    "        _ids_str = [str(_id) for _id in _ids]\n",
    "        _nids = [df.loc[_id].id for _id in _ids]\n",
    "        _dict[max(_nids)] = \" \".join(_ids_str)\n",
    "    return _dict\n",
    "\n",
    "\n",
    "\n",
    "cluster_dict = find_cluster_dict(clu_dict)\n",
    "\n",
    "cluster_dict_dfId = find_cluster_dict_dfId(clu_dict)\n",
    "\n",
    "for _key in cluster_dict_dfId:\n",
    "    print _key, '=>', cluster_dict_dfId[_key]\n",
    "    \n",
    "#for _key in clu_dict:\n",
    "#    print _key, '=>', clu_dict[_key]\n",
    "\n",
    "print dbscan.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168224 --> 1\n",
      "167970 --> 1\n",
      "168011 --> 1\n",
      "168101 --> 2\n",
      "168007 --> 1\n",
      "168267 --> 1\n",
      "168211 --> 1\n",
      "168090 --> 1\n",
      "168215 --> 1\n",
      "168314 --> 1\n",
      "167931 --> 1\n",
      "167804 --> 1\n",
      "168253 --> 1\n",
      "simple:  76.6029930115\n",
      "168224 --> 1\n",
      "167970 --> 1\n",
      "168011 --> 1\n",
      "168101 --> 2\n",
      "168007 --> 1\n",
      "168267 --> 1\n",
      "168211 --> 1\n",
      "168090 --> 2\n",
      "168215 --> 1\n",
      "168314 --> 1\n",
      "167931 --> 1\n",
      "167804 --> 1\n",
      "168253 --> 1\n",
      "detect_emtion: 76.4649848938\n"
     ]
    }
   ],
   "source": [
    "#detect the emotion of cluster\n",
    "\n",
    "from snownlp import SnowNLP\n",
    "import time\n",
    "\n",
    "def detect_emotion_simple(dfidlist):\n",
    "    total_text = \"\"\n",
    "    for dfid in dfidlist:\n",
    "        if type(dfid) == str:\n",
    "            dfid = int(dfid)\n",
    "        total_text += df.loc[dfid].content\n",
    "    _e = SnowNLP(total_text).sentiments\n",
    "    if _e > 0.5:\n",
    "        return 1\n",
    "    elif _e == 0.5:\n",
    "        return 3\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "t0 = time.time()    \n",
    "for _key in cluster_dict_dfId:\n",
    "    print _key, '-->', detect_emotion_simple(cluster_dict_dfId[_key].split())\n",
    "t1 = time.time()\n",
    "print 'simple: ', t1-t0\n",
    "\n",
    "def detect_emotion(dfidlist):\n",
    "    _p = 0\n",
    "    _n = 0\n",
    "    for dfid in dfidlist:\n",
    "        if type(dfid) == str:\n",
    "            dfid = int(dfid)\n",
    "        _c = df.loc[dfid].content\n",
    "        _t = df.loc[dfid].title\n",
    "        _e = SnowNLP(_c).sentiments\n",
    "        _es = 0\n",
    "        if _e > 0.5:\n",
    "            _es = 1\n",
    "            _p += 1\n",
    "        else:\n",
    "            _es = 2\n",
    "            _n += 1\n",
    "#         print _t, \"-->\", _es\n",
    "    if _p > _n:\n",
    "        return 1\n",
    "    elif _p == _n:\n",
    "        return 3\n",
    "    else:\n",
    "        return 2\n",
    "t2 = time.time()\n",
    "for _key in cluster_dict_dfId:\n",
    "    print _key, '-->', detect_emotion(cluster_dict_dfId[_key].split())\n",
    "t3 = time.time()\n",
    "print 'detect_emtion:', t3-t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#insert into db\n",
    "cur = conn.cursor()\n",
    "sql = \"insert into newsCluster(centerId, cluster, hot, lastupdate, emotion) values(%s, %s, %s, %s, %s)\"\n",
    "#print cur.execute(sql)\n",
    "\n",
    "import time\n",
    "today = time.strftime('%Y-%m-%d', time.localtime())\n",
    "\n",
    "datas = []\n",
    "for _center_id in cluster_dict:\n",
    "    _ids = cluster_dict[_center_id]\n",
    "    _data = (_center_id, _ids, len(_ids.split()), today)\n",
    "    datas.append(_data)\n",
    "\n",
    "cur.executemany(sql, datas)\n",
    "cur.close()\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print dbscan.components_[144]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35家化妆品企业遭点名 部分知名微商品牌上榜 ◎每经记者 金喆 市场空间较为艰难的微商又有了新麻烦。 1月9日，广东省食品药品监督管理局发布化妆品生产企业监督检查结果通告，在抽检的118家化妆品生产企业中发现35家未履行质量安全责任。 《每日经济新闻》记者注意到，微商知名品牌俏十岁和思埠集团子品牌黛莱美均上榜。 俏十岁、黛莱美上榜 上述通告称，2015年11月16~27日，广东省食品药品监督管理局开展了全省化妆品执法检查，主要检查生产企业产品注册备案和生产记录情况，共抽查118家化妆品生产企业，发现35家化妆品生产企业未履行好质量安全主体责任。 记者在通告中看到，广州俏十岁生物科技有限公司被抽查的5个国产非特殊用途化妆品，备案归档资料均缺少产品安全性风险评估报告；广州黛莱美化妆品有限公司被抽查的2个国产非特殊用途化妆品，产品标签标识不规范。上述两家公司均被广州市食品药品监督管理局责令改正。据记者了解，俏十岁和黛莱美早期都以面膜产品起家。 此外，妮维雅、广州好迪集团有限公司等33家公司也被点名。 工商资料显示，广州俏十岁生物科技有限公司成立于2015年2月，法定代表人为武斌；广州黛莱美化妆品有限公司成立于2011年3月，法定代表人为吴召国。武斌、吴召国都是网络上的红人，他们被很多微商从业人员视为鼻祖，短期内上演的暴富神话被津津乐道或当做励志故事。 现在，黛莱美已经不止卖面膜，还有洗面奶、精华液等多种化妆品。一位业内人士说，虽然现在微商已经没有之前那么火了，但黛莱美这样的品牌还是能借助思埠的影响力在线下走量。 传统微商面临信任危机 微商“大哥大”被点名，折射出微商行业诸多不规范问题。微商起家产品——面膜更是问题频出。 2015年4月，90后“网红”周梦晗被媒体曝光留学回国后营造“网红”身份，积累10万粉丝并售卖面膜，同时也发展下线，自称年收入近8位数。而她的三无面膜却让一些买家差点毁了容。 “取个高大上的公司名字，然后工商注册、产品设计，再买现成的面膜贴牌，以前很多微商都是这么干的。”前述业内人士说，早期野蛮生长的过程中，大部分微商的品牌与厂家是断开的。而在广州白云区周边，聚集着大量面膜代加工的小厂家。 第三方微信营销平台微信通CEO王易也曾告诉记者，面膜的生产成本和包装等费用很低，基本在2~3元左右，产量越大稀释到每张面膜上的固定成本越少。但“高大上”的产品摆到朋友圈后经过代理层层加价翻倍，最下级的到手价可能已达到30元。 2015年4月，央视在曝光“毒面膜”时也提到，广州市白云区有大量的化妆品OEM公司提供全套的产品方案。吴召国也曾对媒体坦承，大部分的微商品牌，没有研发，不管质量，完全交由上游代工厂做，只管渠道。 在以王易为代表的观察者眼里，缺乏质量保障的面膜正是导致微商遭遇滑铁卢的关键所在。分销平台有量业务负责人裴大鹏告诉记者，微商应通过社交网络中的口碑传播和信任关系影响周围的消费者，品牌商需要做好产品端的服务。 (每日经济新闻) 自动播放开关 自动播放 北京消协 多品牌化妆品含有重金属 正在加载... < > 更多精彩内容欢迎搜索关注微信公众号：腾讯财经（financeapp）。\n",
      "0.999999998775\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-c11749929101>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0m_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_content\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0m_sscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSnowNLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiments\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_sscore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#         print '正面: ',  _content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/snownlp/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbm25\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbm25\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBM25\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/snownlp/sim/bm25.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgdl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m0.0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "#Test emotion\n",
    "from snownlp import SnowNLP\n",
    "_text = df.content.head(100)[0]\n",
    "print _text\n",
    "s = SnowNLP(_text)\n",
    "print s.sentiments\n",
    "\n",
    "_p = 0\n",
    "_n = 0\n",
    "for _content in df.content.head(100):\n",
    "    _sscore = SnowNLP(_content).sentiments\n",
    "    if _sscore > 0.5:\n",
    "#         print '正面: ',  _content\n",
    "        _p +=1\n",
    "    else:\n",
    "#         print '负面 ===>', _content\n",
    "        _n += 1\n",
    "print _p, _n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309  :  今起暂停实施熔断机制 证监会称将总结经验教训\n",
      "313  :  4天内4次熔断 A股创单日最短交易时间纪录\n",
      "315  :  今起暂停熔断机制 机构齐称超预期\n",
      "468  :  今起暂停实施熔断机制 证监会称将总结经验教训\n",
      "471  :  宋清辉：熔断机制暂停 A股下一步需解决啥问题\n",
      "472  :  宋清辉：熔断机制有必要但有时多余\n",
      "480  :  收评：熔断存废之间 沪指新年首周大跌10%创4个月来最差(组图)\n",
      "733  :  沪深股市触发熔断机制\n",
      "757  :  A股首次熔断除了提前收盘 还发生6件新鲜事\n",
      "774  :  反思熔断：美国也曾经历怀疑与变革\n",
      "782  :  A股首次熔断发生后 市场热议提出四大建议\n",
      "789  :  熔断重启时间未定 证监会称将“不断完善相关机制”\n",
      "907  :  收盘：A股放量巨幅震动 沪指反转微跌3200失而复得\n",
      "927  :  新年首个交易日A股暴跌7% 两度触发熔断机制\n",
      "961  :  A股创最快“下班”纪录 新年开市4天2天提前收盘\n"
     ]
    }
   ],
   "source": [
    "def debug_title(id_list):\n",
    "    for _id in id_list:\n",
    "        if type(_id) == str:\n",
    "            _id = int(_id)\n",
    "        print _id, \" : \", df.loc[_id].title\n",
    "        \n",
    "debug_title('309 313 315 468 471 472 480 733 757 774 782 789 907 927 961'.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
